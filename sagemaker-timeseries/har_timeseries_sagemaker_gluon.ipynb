{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Timeseries SageMaker Template with Gluon\n",
    "\n",
    "This is a template to run the human activity recognition notebook. Refer the `smartphone_human_activity_classification_gluon.ipynb` for non sagemaker version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from mxnet import gluon\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Package Data\n",
    "\n",
    "1. Load your train and test data as numpy arrays\n",
    "2. Package data as a pickle file and upload to S3\n",
    "\n",
    "by doing this, we can use the generic_ts.py file to run any timeseries classification task with SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def get_labels_from_csv(path):\n",
    "    values = []\n",
    "    with open(path, 'rb') as csvfile:\n",
    "        rd = csv.reader(csvfile, delimiter=',')\n",
    "        for row in rd:\n",
    "            values.append(float(row[0]))\n",
    "    return np.array(values).astype('float32')\n",
    "\n",
    "\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "LABELS = [\n",
    "    \"WALKING\",\n",
    "    \"WALKING_UPSTAIRS\",\n",
    "    \"WALKING_DOWNSTAIRS\",\n",
    "    \"SITTING\",\n",
    "    \"STANDING\",\n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "\n",
    "path = '../data'\n",
    "\n",
    "train = [path + \"/har_data/train/%strain.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "test = [path + \"/har_data/test/%stest.txt\" % signal for signal in INPUT_SIGNAL_TYPES]\n",
    "\n",
    "\n",
    "def load_data(files):\n",
    "    arr = []\n",
    "    for fname in files:\n",
    "        with open(fname, 'r') as f:\n",
    "            rows = [row.replace('  ', ' ').strip().split(' ') for row in f]\n",
    "            arr.append([np.array(ele, dtype=np.float32) for ele in rows])\n",
    "    return np.transpose(np.array(arr), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 128, 9), (2947, 128, 9))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = load_data(train)\n",
    "X_test = load_data(test)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_path = path + \"/har_data/train/y_train.txt\"\n",
    "y_train = get_labels_from_csv(y_train_path)\n",
    "\n",
    "y_test_path = path + \"/har_data/test/y_test.txt\"\n",
    "y_test = get_labels_from_csv(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 5.0, 0.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "print(min(y_train), max(y_train), min(y_test), max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 4: cd..: command not found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir pkl_data\n",
    "cd pkl_data\n",
    "mkdir train\n",
    "mkdir test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([X_train, y_train], open('pkl_data/train/train.pkl', 'wb'))\n",
    "pickle.dump([X_test, y_test], open('pkl_data/test/test.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2947, 128, 9), (2947,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t, y_t = pickle.load(open('pkl_data/test/test.pkl', \"rb\"))\n",
    "#X_t, y_t = pickle.load(open('test.pkl', \"rb\"))\n",
    "X_t.shape, y_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- end of data transformation, loading and packaging ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading the data\n",
    "\n",
    "We use the `sagemaker.Session.upload_data` function to upload our datasets to an S3 location. The return value `inputs` identifies the location -- we will use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-148886336128\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='pkl_data', key_prefix='data/har_pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execute cell below to view the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'''\r\n",
      "TimeSeries Classificataion SageMaker Template \r\n",
      "'''\r\n",
      "\r\n",
      "from __future__ import print_function\r\n",
      "\r\n",
      "import logging\r\n",
      "import mxnet as mx\r\n",
      "from mxnet import gluon, autograd, nd\r\n",
      "from mxnet.gluon import nn\r\n",
      "import numpy as np\r\n",
      "import json\r\n",
      "import time\r\n",
      "import math\r\n",
      "import pickle\r\n",
      "\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "\r\n",
      "import os\r\n",
      "def find_file(root_path, file_name):\r\n",
      "    for root, dirs, files in os.walk(root_path):\r\n",
      "        if file_name in files:\r\n",
      "            return os.path.join(root, file_name)\r\n",
      "\r\n",
      "def detach(hidden):\r\n",
      "    if isinstance(hidden, (tuple, list)):\r\n",
      "        hidden = [i.detach() for i in hidden]\r\n",
      "    else:\r\n",
      "        hidden = hidden.detach()\r\n",
      "    return hidden\r\n",
      "\r\n",
      "class BaseRNNClassifier(mx.gluon.Block):\r\n",
      "    '''\r\n",
      "    Exensible RNN class with LSTM that can operate with MXNet NDArray iter or DataLoader.\r\n",
      "    Includes fit() function to mimic the symbolic fit() function\r\n",
      "    '''\r\n",
      "    \r\n",
      "    @classmethod\r\n",
      "    def get_data(cls, batch, iter_type, ctx):\r\n",
      "        ''' get data and label from the iterator/dataloader '''\r\n",
      "        if iter_type == 'mxiter':\r\n",
      "            X = batch.data[0].as_in_context(ctx)\r\n",
      "            y = batch.label[0].as_in_context(ctx)\r\n",
      "        elif iter_type in [\"numpy\", \"dataloader\"]:\r\n",
      "            X = batch[0].as_in_context(ctx)\r\n",
      "            y = batch[1].as_in_context(ctx)\r\n",
      "        else:\r\n",
      "            raise ValueError(\"iter_type must be mxiter or numpy\")\r\n",
      "        return X, y\r\n",
      "    \r\n",
      "    @classmethod\r\n",
      "    def get_all_labels(cls, data_iterator, iter_type):\r\n",
      "        if iter_type == 'mxiter':\r\n",
      "            pass\r\n",
      "        elif iter_type in [\"numpy\", \"dataloader\"]:\r\n",
      "            return data_iterator._dataset._label\r\n",
      "    \r\n",
      "    def __init__(self, ctx):\r\n",
      "        super(BaseRNNClassifier, self).__init__()\r\n",
      "        self.ctx = ctx\r\n",
      "        self.batch_size = 128\r\n",
      "\r\n",
      "    def build_model(self, n_out, rnn_size=128, n_layer=1):\r\n",
      "        self.rnn_size = rnn_size\r\n",
      "        self.n_layer = n_layer\r\n",
      "        self.n_out = n_out\r\n",
      "        \r\n",
      "        # LSTM default; #TODO(Sunil): make this generic\r\n",
      "        self.lstm = mx.gluon.rnn.LSTM(self.rnn_size, self.n_layer, layout='NTC')\r\n",
      "        #self.lstm = mx.gluon.rnn.GRU(self.rnn_size, self.n_layer)\r\n",
      "        self.output = mx.gluon.nn.Dense(self.n_out)\r\n",
      "\r\n",
      "    def forward(self, x, hidden):\r\n",
      "        out, hidden = self.lstm(x, hidden)\r\n",
      "        out = out[:, out.shape[1]-1, :]\r\n",
      "        out = self.output(out)\r\n",
      "        return out, hidden\r\n",
      "    \r\n",
      "    def save(self, model_dir):\r\n",
      "        # save the model\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, self.batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "\r\n",
      "        #TODO(Sunil): Update when HybridSequential/hybridize is available for RNN/LSTM\r\n",
      "        #y = self.forward(mx.sym.var('data'), hidden)\r\n",
      "        #y.save('%s/model.json' % model_dir)\r\n",
      "        self.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "    def compile_model(self, loss=None, lr=3E-3):\r\n",
      "        self.collect_params().initialize(mx.init.Xavier(), ctx=self.ctx)\r\n",
      "        self.criterion = mx.gluon.loss.SoftmaxCrossEntropyLoss()\r\n",
      "        self.loss = mx.gluon.loss.SoftmaxCrossEntropyLoss() if loss is None else loss\r\n",
      "        self.lr = lr\r\n",
      "        self.optimizer = mx.gluon.Trainer(self.collect_params(), 'adam', \r\n",
      "                                          {'learning_rate': self.lr})\r\n",
      "\r\n",
      "    def top_k_acc(self, data_iterator, iter_type='mxiter', top_k=3, batch_size=128):\r\n",
      "        self.batch_size = batch_size\r\n",
      "        batch_pred_list = []\r\n",
      "        true_labels = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            batch_pred = self.forward(data, hidden)\r\n",
      "            #batch_pred = mx.nd.argmax(batch_pred, axis=1)\r\n",
      "            batch_pred_list.append(batch_pred.asnumpy())\r\n",
      "            true_labels.append(label)\r\n",
      "        y = np.vstack(batch_pred_list)\r\n",
      "        true_labels = np.vstack(true_labels)\r\n",
      "        argsorted_y = np.argsort(y)[:,-top_k:]\r\n",
      "        return np.asarray(np.any(argsorted_y.T == true_labels, axis=0).mean(dtype='f'))\r\n",
      "    \r\n",
      "    def evaluate_accuracy(self, data_iterator, metric='acc', iter_type='mxiter', batch_size=128):\r\n",
      "        met = mx.metric.Accuracy()\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            # Lets do a forward pass only!\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            preds = mx.nd.argmax(output, axis=1)\r\n",
      "            met.update(labels=label, preds=preds)\r\n",
      "                \r\n",
      "        #if self.all_labels is None:\r\n",
      "        #    self.all_labels = BaseRNNClassifier.get_all_labels(data_iterator, iter_type)\r\n",
      "        #preds = self.predict(data_iterator, iter_type=iter_type, batch_size=batch_size)\r\n",
      "        #met.update(labels=mx.nd.array(self.all_labels[:len(preds)]), preds=preds)\r\n",
      "        \r\n",
      "        return met.get()                   \r\n",
      "                    \r\n",
      "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\r\n",
      "        batch_pred_list = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            batch_pred_list.append(output.asnumpy())\r\n",
      "        #return np.vstack(batch_pred_list)\r\n",
      "        return np.argmax(np.vstack(batch_pred_list), 1)\r\n",
      "    \r\n",
      "    def fit(self, train_data, test_data, epochs, batch_size, verbose=True):\r\n",
      "        '''\r\n",
      "        @train_data:  can be of type list of Numpy array, DataLoader, MXNet NDArray Iter\r\n",
      "        '''\r\n",
      "        \r\n",
      "        self.batch_size = batch_size\r\n",
      "        moving_loss = 0.\r\n",
      "        total_batches = 0\r\n",
      "\r\n",
      "        train_loss = []\r\n",
      "        train_acc = []\r\n",
      "        test_acc = []\r\n",
      "\r\n",
      "        iter_type = 'numpy'\r\n",
      "        train_iter = None\r\n",
      "        test_iter = None\r\n",
      "        #print \"Data type:\", type(train_data), type(test_data), iter_type, type(train_data[0])\r\n",
      "        \r\n",
      "        # Can take MX NDArrayIter, or DataLoader\r\n",
      "        if isinstance(train_data, mx.io.NDArrayIter):\r\n",
      "            train_iter = train_data\r\n",
      "            test_iter = test_data\r\n",
      "            iter_type = 'mxiter'\r\n",
      "            #total_batches = train_iter.num_data // train_iter.batch_size\r\n",
      "\r\n",
      "        elif isinstance(train_data, list):\r\n",
      "            if isinstance(train_data[0], np.ndarray) and isinstance(train_data[1], np.ndarray):\r\n",
      "                X, y = np.asarray(train_data[0]).astype('float32'), np.asarray(train_data[1]).astype('float32')\r\n",
      "                tX, ty = np.asarray(test_data[0]).astype('float32'), np.asarray(test_data[1]).astype('float32')\r\n",
      "                \r\n",
      "                total_batches = X.shape[0] // batch_size\r\n",
      "                train_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(X, y), \r\n",
      "                                    batch_size=batch_size, shuffle=True, last_batch='discard')\r\n",
      "                test_iter = mx.gluon.data.DataLoader(mx.gluon.data.ArrayDataset(tX, ty), \r\n",
      "                                    batch_size=batch_size, shuffle=False, last_batch='discard')\r\n",
      "                \r\n",
      "        elif isinstance(train_data, mx.gluon.data.dataloader.DataLoader) and isinstance(test_data, mx.gluon.data.dataloader.DataLoader):\r\n",
      "            train_iter = train_data\r\n",
      "            test_iter = test_data\r\n",
      "            total_batches = len(train_iter)\r\n",
      "        else:\r\n",
      "            raise ValueError(\"pass mxnet ndarray or numpy array as [data, label]\")\r\n",
      "\r\n",
      "        #print \"Data type:\", type(train_data), type(test_data), iter_type\r\n",
      "        #print \"Sizes\", self.n_layer, batch_size, self.rnn_size, self.ctx\r\n",
      "        \r\n",
      "        for e in range(epochs):\r\n",
      "            #print self.lstm.collect_params()\r\n",
      "\r\n",
      "            # reset iterators if of MXNet Itertype\r\n",
      "            if iter_type == \"mxiter\":\r\n",
      "                train_iter.reset()\r\n",
      "                test_iter.reset()\r\n",
      "        \r\n",
      "            init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "            hidden = [init_state] * 2                \r\n",
      "            #hidden = self.begin_state(func=mx.nd.zeros, batch_size=batch_size, ctx=self.ctx)\r\n",
      "            yhat = []\r\n",
      "            for i, batch in enumerate(train_iter):\r\n",
      "                data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "                #print \"Data Shapes:\", data.shape, label.shape\r\n",
      "                hidden = detach(hidden)\r\n",
      "                #with mx.gluon.autograd.record(train_mode=True):\r\n",
      "                with autograd.record():\r\n",
      "                    preds, hidden = self.forward(data, hidden)\r\n",
      "                    #print preds[0].shape, hidden[0].shape, label.shape\r\n",
      "                    loss = self.loss(preds, label) \r\n",
      "                    yhat.extend(preds)\r\n",
      "                loss.backward()                                        \r\n",
      "                self.optimizer.step(batch_size)\r\n",
      "                preds = mx.nd.argmax(preds, axis=1)\r\n",
      "                \r\n",
      "                batch_acc = mx.nd.mean(preds == label).asscalar()\r\n",
      "\r\n",
      "                if i == 0:\r\n",
      "                    moving_loss = nd.mean(loss).asscalar()\r\n",
      "                else:\r\n",
      "                    moving_loss = .99 * moving_loss + .01 * mx.nd.mean(loss).asscalar()\r\n",
      "                    \r\n",
      "                if verbose and i%100 == 0:\r\n",
      "                    print('[Epoch {}] [Batch {}/{}] Loss: {:.5f}, Batch acc: {:.5f}'.format(\r\n",
      "                          e, i, total_batches, moving_loss, batch_acc))                    \r\n",
      "                    \r\n",
      "            train_loss.append(moving_loss)\r\n",
      "            \r\n",
      "            t_acc = self.evaluate_accuracy(train_iter, iter_type=iter_type, batch_size=batch_size)\r\n",
      "            train_acc.append(t_acc[1])\r\n",
      "            \r\n",
      "            tst_acc = self.evaluate_accuracy(test_iter, iter_type=iter_type, batch_size=batch_size)\r\n",
      "            test_acc.append(tst_acc[1])\r\n",
      "\r\n",
      "            print(\"Epoch %s. Loss: %.5f Train Acc: %s Test Acc: %s\" % (e, moving_loss, t_acc, tst_acc))\r\n",
      "        return train_loss, train_acc, test_acc\r\n",
      "                    \r\n",
      "    def predict(self, data_iterator, iter_type='mxiter', batch_size=128):\r\n",
      "        batch_pred_list = []\r\n",
      "        init_state = mx.nd.zeros((self.n_layer, batch_size, self.rnn_size), self.ctx)\r\n",
      "        hidden = [init_state] * 2\r\n",
      "        for i, batch in enumerate(data_iterator):\r\n",
      "            data, label = BaseRNNClassifier.get_data(batch, iter_type, self.ctx)\r\n",
      "            output, hidden = self.forward(data, hidden)\r\n",
      "            batch_pred_list.append(output.asnumpy())\r\n",
      "        return np.argmax(np.vstack(batch_pred_list), 1)\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Training methods                                             #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def train(channel_input_dirs, hyperparameters, **kwargs):\r\n",
      "\r\n",
      "    # retrieve the hyperparameters we set in notebook (with some defaults)\r\n",
      "    batch_size = hyperparameters.get('batch_size', 100)\r\n",
      "    epochs = hyperparameters.get('epochs', 10)\r\n",
      "    learning_rate = hyperparameters.get('learning_rate', 0.1)\r\n",
      "    momentum = hyperparameters.get('momentum', 0.9)\r\n",
      "    log_interval = hyperparameters.get('log_interval', 100)\r\n",
      "    num_gpus = hyperparameters.get('num_gpus', 0)\r\n",
      "    \r\n",
      "    # Parametrize the network definition\r\n",
      "    n_out = hyperparameters.get('n_out', 2)\r\n",
      "    rnn_size = hyperparameters.get('rnn_size', 64)\r\n",
      "    n_layer = hyperparameters.get('n_layer', 1)\r\n",
      "\r\n",
      "    X_train, y_train = load_data('train')\r\n",
      "    X_test, y_test = load_data('test')\r\n",
      "\r\n",
      "    # context \r\n",
      "    ctx = mx.cpu()\r\n",
      "    if num_gpus >1:\r\n",
      "        ctx = [mx.gpu(i) for i in range(num_gpus)]\r\n",
      "    \r\n",
      "    print (ctx)\r\n",
      "    model = BaseRNNClassifier(ctx)\r\n",
      "    model.build_model(n_out=n_out, rnn_size=rnn_size, n_layer=n_layer)\r\n",
      "    model.compile_model()\r\n",
      "    train_loss, train_acc, test_acc = model.fit([X_train, y_train], [X_test, y_test], batch_size=batch_size, epochs=epochs)\r\n",
      "    return model\r\n",
      "\r\n",
      "def save(net, model_dir):\r\n",
      "    print (\"saving the model\")\r\n",
      "    net.save(model_dir)\r\n",
      "    \r\n",
      "    # save the model\r\n",
      "    #y = net(mx.sym.var('data'))\r\n",
      "    #net.save('%s/model.json' % model_dir)\r\n",
      "    #net.collect_params().save('%s/model.params' % model_dir)\r\n",
      "\r\n",
      "## Load Train and Test Data\r\n",
      "def load_data(typ):\r\n",
      "    path = \".\"\r\n",
      "    if typ == \"train\":\r\n",
      "        #Load Train Data\r\n",
      "        f = find_file(path, \"train.pkl\")\r\n",
      "    else:#Load Test Data\r\n",
      "        f = find_file(path, \"test.pkl\")\r\n",
      "    X_t, y_t = pickle.load(open(f, \"rb\"))\r\n",
      "    return X_t, y_t\r\n",
      "\r\n",
      "\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "# Hosting methods                                              #\r\n",
      "# ------------------------------------------------------------ #\r\n",
      "\r\n",
      "def model_fn(model_dir):\r\n",
      "    \"\"\"\r\n",
      "    Load the gluon model. Called once when hosting service starts.\r\n",
      "\r\n",
      "    :param: model_dir The directory where model files are stored.\r\n",
      "    :return: a model (in this case a Gluon network)\r\n",
      "    \"\"\"\r\n",
      "    symbol = mx.sym.load('%s/model.json' % model_dir)\r\n",
      "    outputs = mx.symbol.softmax(data=symbol, name='softmax_label')\r\n",
      "    inputs = mx.sym.var('data')\r\n",
      "    param_dict = gluon.ParameterDict('model_')\r\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\r\n",
      "    net.load_params('%s/model.params' % model_dir, ctx=mx.cpu())\r\n",
      "    return net\r\n",
      "\r\n",
      "\r\n",
      "def transform_fn(net, data, input_content_type, output_content_type):\r\n",
      "    \"\"\"\r\n",
      "    Transform a request using the Gluon model. Called once per request.\r\n",
      "\r\n",
      "    :param net: The Gluon model.\r\n",
      "    :param data: The request payload.\r\n",
      "    :param input_content_type: The request content type.\r\n",
      "    :param output_content_type: The (desired) response content type.\r\n",
      "    :return: response payload and content type.\r\n",
      "    \"\"\"\r\n",
      "    # we can use content types to vary input/output handling, but\r\n",
      "    # here we just assume json for both\r\n",
      "    parsed = json.loads(data)\r\n",
      "    nda = mx.nd.array(parsed)\r\n",
      "    output = net(nda)\r\n",
      "    prediction = mx.nd.argmax(output, axis=1)\r\n",
      "    response_body = json.dumps(prediction.asnumpy().tolist()[0])\r\n",
      "    return response_body, output_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!cat generic_ts.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "\n",
    "The ```MXNet``` class allows us to run our training function on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we will run our training job on a single m4.xlarge instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = MXNet(\"generic_ts.py\", \n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=\"ml.p2.xlarge\",\n",
    "          hyperparameters={'batch_size': 32, \n",
    "                         'epochs': 1, \n",
    "                         'learning_rate': 0.01, \n",
    "                         'momentum': 0.9, \n",
    "                         'log_interval': 100,\n",
    "                         'n_out': len(LABELS),\n",
    "                         'num_gpus': 1\n",
    "                          })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-148886336128\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-mxnet-py2-gpu-2018-04-25-21-49-55-872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................\n",
      "\u001b[31mexecuting startup script (first run)\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:48,222 INFO - root - running container entrypoint\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:48,223 INFO - root - starting train task\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:49,930 INFO - mxnet_container.train - MXNetTrainingEnvironment: {'enable_cloudwatch_metrics': False, 'available_gpus': 1, 'channels': {u'training': {u'TrainingInputMode': u'File', u'RecordWrapperType': u'None', u'S3DistributionType': u'FullyReplicated'}}, '_ps_verbose': 0, 'resource_config': {u'current_host': u'algo-1', u'hosts': [u'algo-1']}, 'user_script_name': u'generic_ts.py', 'input_config_dir': '/opt/ml/input/config', 'channel_dirs': {u'training': u'/opt/ml/input/data/training'}, 'code_dir': '/opt/ml/code', 'output_data_dir': '/opt/ml/output/data/', 'output_dir': '/opt/ml/output', 'model_dir': '/opt/ml/model', 'hyperparameters': {u'sagemaker_program': u'generic_ts.py', u'num_gpus': 1, u'learning_rate': 0.01, u'log_interval': 100, u'epochs': 1, u'batch_size': 32, u'sagemaker_region': u'us-east-1', u'sagemaker_enable_cloudwatch_metrics': False, u'n_out': 6, u'sagemaker_job_name': u'sagemaker-mxnet-py2-gpu-2018-04-25-21-49-55-872', u'sagemaker_container_log_level': 20, u'momentum': 0.9, u'sagemaker_submit_directory': u's3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-py2-gpu-2018-04-25-21-49-55-872/source/sourcedir.tar.gz'}, 'hosts': [u'algo-1'], '_scheduler_ip': '10.32.0.4', '_ps_port': 8000, 'user_script_archive': u's3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-py2-gpu-2018-04-25-21-49-55-872/source/sourcedir.tar.gz', '_scheduler_host': u'algo-1', 'sagemaker_region': u'us-east-1', 'input_dir': '/opt/ml/input', 'user_requirements_file': None, 'current_host': u'algo-1', 'container_log_level': 20, 'available_cpus': 4, 'base_dir': '/opt/ml'}\u001b[0m\n",
      "\u001b[31mDownloading s3://sagemaker-us-east-1-148886336128/sagemaker-mxnet-py2-gpu-2018-04-25-21-49-55-872/source/sourcedir.tar.gz to /tmp/script.tar.gz\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:50,070 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTP connection (1): 169.254.170.2\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:50,197 INFO - botocore.vendored.requests.packages.urllib3.connectionpool - Starting new HTTPS connection (1): s3.amazonaws.com\u001b[0m\n",
      "\u001b[31m2018-04-25 21:55:50,313 INFO - mxnet_container.train - Starting distributed training task\u001b[0m\n",
      "\u001b[31mgpu(0)\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 0/229] Loss: 1.76797, Batch acc: 0.40625\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 100/229] Loss: 1.37346, Batch acc: 0.37500\u001b[0m\n",
      "\u001b[31m[Epoch 0] [Batch 200/229] Loss: 1.19873, Batch acc: 0.28125\u001b[0m\n",
      "\u001b[31mEpoch 0. Loss: 1.11005 Train Acc: ('accuracy', 0.5057314410480349) Test Acc: ('accuracy', 0.48947010869565216)\u001b[0m\n",
      "\u001b[31msaving the model\u001b[0m\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "m.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p27",
   "language": "python",
   "name": "conda_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
